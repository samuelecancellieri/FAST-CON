/* Copyright (C) 2010 Universita' degli Studi di Roma La Sapienza          */
/*                                                                         */

#define GRAPHGEN_DISTRIBUTED_MEMORY
#define GRAPH_GENERATOR_MPI

/* These need to be before any possible inclusions of stdint.h or inttypes.h.
 * */
#ifndef __STDC_LIMIT_MACROS
#define __STDC_LIMIT_MACROS
#endif
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

#include <math.h>
#include <mpi.h>
#include <assert.h>
#include <string.h>
#include <stdlib.h>
#include <stdio.h>
#include <time.h>
#include <getopt.h>
#include <limits.h>
#include <stddef.h>
#include <stdint.h>
#include <inttypes.h>
#include <cuda_runtime.h>
/* graph500 header files (modified) */
#include "../generator/make_graph.h"
#include "reference_common.h"
/* my header files */
#include "header.h"
#include "gputils.h"
#include "cputils.h"
#include "make_struct.h"
#include "make_struct_gpufunc.h"
#include "make_stcon.h"
#include "adj_func.h"

#include "load_graph.h"

#define BUILD_STRUCT_DEVICE 1

#define SEED1 23
#define SEED2 24
#define NUM_BFS_GRAPH500 100

#define DEBUG_DIR "data"
#define CODEVERS "5.7 2014-01-14"

FILE *fp_struct = NULL;
FILE *fp_bfs = NULL;
FILE *fp_time = NULL;
FILE *fp_stats = NULL;
int nthreads;
int maxblocks;
int rank, size;

#ifdef SIZE_MUST_BE_A_POWER_OF_TWO
int lgsize;
#endif

int64_t MaxLabel;
int64_t MaxGlobalLabel;
int global_scale;
double global_edgefactor;
int dbg_lvl;
size_t freed_memory_size;
size_t current_allocated_size;
int num_bfs_roots;
int validation = 1; // default is validation is ON
int genGraph = 1;   // default is graph is generated

INT_T red_root = -1, blue_root = -1;

double d_send_size_factor = 1;
double d_recv_size_factor = 2;
double d_mask_size_factor = 2;
unsigned int green_exe_time = 0;
#ifdef BUILD_STRUCT_DEVICE
#define NUM_NVERTS_REQ 200 // Number of arrays of Nverts size required in Device Memory
#else
#define NUM_NVERTS_REQ (60 + 8 * size + 4) // Assuming that thrust calls needs 4 nverts arrays
#endif
int main(int argc, char **argv)
{
	// Init MPI env
	MPI_Init(&argc, &argv);
	// Setup global variables
	setup_globals_MPI();

	// Timing
	double prepStart, prepStop, prepTime;
	prepStart = MPI_Wtime();

	// Vars
	int SCALE;				// scale of the problem N=2^SCALE
	double edgefactor;		// M = N*edgefactor
	int64_t nedges = 0;		// number of edges generated by each proc
	int64_t *edges;			// the array of edges
	uint64_t seed1, seed2;  // seeds of the randoom number generator
	char fname_struct[256]; // output debug file for make_struct
	char fname_bfs[256];	// output debug file for make_bfs
	char fname_stats[256];  // output file for statistics
	char fname_time[256];   // output debug file for time information
	int64_t cc;				// counter
	int opt = 0, errflg = 0;

	int gpu_id = 0;

	// Timing
	double start, stop;
	double make_struct_time;
	double make_graph_time = 0;
	double run_make_bfs_time;

	// Initialize Vars
	SCALE = 16;
	edgefactor = 16.;
	srand(time(0));
	seed1 = rand();
	seed2 = rand();
	nthreads = BDIM;
	maxblocks = MAXBLOCKS;
	num_bfs_roots = NUM_BFS_GRAPH500;
	current_allocated_size = 0;

	char filename_graph[256];
	char output_dir[256] = DEBUG_DIR;
	int genType = 0; // 0 = RMAT - 1 = RANDOM

	// Command line option -S SCALE -E Edgefactor -t threads -b blocks -1 seed1 -2 seed2 -N Numbfsroots
	// -D gpu_device_id
	// Scan command line
	while ((opt = getopt(argc, argv, "S:E:t:b:1:2:N:V:r:m:s:T:D:f:d:B:R:G:v")) != -1)
	{
		switch (opt)
		{
		case 'R':
			red_root = atol(optarg);
			break;
		case 'B':
			blue_root = atol(optarg);
			break;
		case 'S':
			SCALE = atoi(optarg);
			break;
		case 'E':
			edgefactor = atof(optarg);
			break;
		case 't':
			nthreads = atoi(optarg);
			break;
		case 'b':
			maxblocks = atoi(optarg);
			break;
		case '1':
			seed1 = atoi(optarg);
			break;
		case '2':
			seed2 = atoi(optarg);
			break;
		case 'N':
			num_bfs_roots = atoi(optarg);
			break;
		case 'T':
			green_exe_time = atoi(optarg);
			break;
		case 'D':
			gpu_id = atoi(optarg);
			break;
		case 'r':
			d_recv_size_factor = atof(optarg);
			if (d_recv_size_factor <= 0)
			{
				fprintf(stderr, "Option -r requires a positive value\n");
				errflg++;
			}
			break;
		case 's':
			d_send_size_factor = atof(optarg);
			if (d_send_size_factor <= 0)
			{
				fprintf(stderr, "Option -s requires a positive value\n");
				errflg++;
			}
			break;
		case 'm':
			d_mask_size_factor = atof(optarg);
			if (d_mask_size_factor <= 0)
			{
				fprintf(stderr, "Option -m requires a positive value\n");
				errflg++;
			}
			break;
		case 'V':
			validation = atoi(optarg);
			break;
		case 'f':
			strcpy(filename_graph, optarg);
			genGraph = 0;
			break;
		case 'd':
			strcpy(output_dir, optarg);
			break;
		case 'G':
			genType = atoi(optarg);
			break;
		case ':': /* option without operand */
			fprintf(stderr, "Option -%c requires an operand\n", optopt);
			errflg++;
			break;
		case '?':
			fprintf(stderr, "Unrecognized option: -%c\n", optopt);
			errflg++;
			break;
		case 'v':
			printf("Version %s\n", CODEVERS);
			return 0;
		default:
			abort();
		}
	}

	if (SCALE <= 0 || edgefactor <= 0 || errflg > 0)
	{
		if (rank == 0)
		{
			fprintf(stderr,
					"Usage: %s -S SCALE -E Edgefactor -1 seed1 -2 seed2 -t nthreads"
					" -b maxblocks -N num_bfs_roots -V (on | off) -T time"
					" -s SendSizeFactor -r ReceiveSizeFactor -m MaskSizeFactor\n"
					"SCALE = log_2(# vertices) [integer, required]\n"
					"  edgefactor = (# edges) / (# vertices) ="
					" .5 * (average vertex degree) [float, defaults to 16]\n"
					"(Random number seed and Kronecker initiator are in main.c)\n"
					"-V num (Maximum number of matching nodes to validate, default 1)\n"
					"-D GPU device ID when running on 1 process (default is 0)\n"
					"-T time in seconds used for green Graph500 (default is 0)\n"
					"-f filename to run on the real graph stored in filename\n"
					"-d directory to store statistics and debug files\n"
					"-G (0|1) 0 for RMAT graph and 1 for Random graph\n"
					"-R vertex -B vertex (red and blue vertices)\n"
					" ????SizeFactor float number > 0 that is multiplied to n_melemes to calculate buffer size\n",
					argv[0]);
		}
		MPI_Abort(MPI_COMM_WORLD, 1);
	}

	if ((red_root != -1) && (blue_root != -1))
	{
		num_bfs_roots = 1;
	}
	else
	{
		red_root = blue_root = -1;
	}

	if (rank == 0)
	{
		fprintf(stdout, "SCALE=%d, Edgefactor=%.1f, Thread=%d, Maxblocks=%d, Num ST-CON=%d, Validations=%d\n"
						"d_send_size_factor=%.1f, d_mask_size_factor=%.1f, d_recv_size_factor=%.1f\n"
						"Filename=%s, graph type=%s\n",
				SCALE, edgefactor, nthreads, maxblocks, num_bfs_roots, validation,
				d_send_size_factor, d_mask_size_factor, d_recv_size_factor,
				(genGraph ? "Generated Graph" : filename_graph),
				(genType ? "Random Graph" : "RMAT Graph"));
		fprintf(stdout, "seed1=%" PRIu64 ", seed2=%" PRIu64 "\n", seed1, seed2);
	}

	// Debug level
#if defined(DBG_LEVEL)
	dbg_lvl = (DBG_LEVEL);
	printf("DEBUG MOD ON. Debug level is %d\n", dbg_lvl);
#else
	printf("DEBUG MOD OFF. Debug level is %d\n", dbg_lvl);
	dbg_lvl = 0;
#endif
	// Debug files (global variables)
	// make_struct debug file
	if (dbg_lvl > 0)
	{
		snprintf(fname_struct, 256, "%s/struct_debug_%d", output_dir, rank);
		fp_struct = fopen(fname_struct, "w");
		if (fp_struct == NULL)
		{
			fprintf(stderr, "Error opening file %s", fname_struct);
			MPI_Abort(MPI_COMM_WORLD, 1);
		}
		// make_bfs debug file
		snprintf(fname_bfs, 256, "%s/bfs_debug_%d", output_dir, rank);
		fp_bfs = fopen(fname_bfs, "w");
		if (fp_bfs == NULL)
		{
			fprintf(stderr, "Error opening file %s", fname_bfs);
			MPI_Abort(MPI_COMM_WORLD, 1);
		}
		snprintf(fname_time, 256, "%s/time_time_debug_%d", output_dir, rank);
		fp_time = fopen(fname_time, "w");
		if (fp_time == NULL)
		{
			fprintf(stderr, "Error opening file %s", fname_time);
			MPI_Abort(MPI_COMM_WORLD, 1);
		}
	}

	// Statistical information
	if (rank == 0)
	{

		time_t rawtime;
		struct tm *timeinfo;
		char buffer[80];
		time(&rawtime);
		timeinfo = localtime(&rawtime);
		strftime(buffer, 80, "%Y%m%d_%H%M%S", timeinfo);

		snprintf(fname_stats, 256, "%s/st_stats_%s", output_dir, buffer);
		fp_stats = fopen(fname_stats, "w");
		if (fp_stats == NULL)
		{
			fprintf(stderr, "Error opening file %s", fname_stats);
			MPI_Abort(MPI_COMM_WORLD, 1);
		}

		fprintf(fp_stats, "SCALE %d\n", SCALE);
		fprintf(fp_stats, "Edgefactor %.1f\n", edgefactor);
		fprintf(fp_stats, "MPI-Nodes %d\n", size);
		fprintf(fp_stats, "Num-ST-CON=%d\n", num_bfs_roots);
		if (genGraph)
			fprintf(fp_stats, "Graph-type %s\n", (genType ? "Random" : "RMAT"));
		else
			fprintf(fp_stats, "Filename %s\n", filename_graph);

		fprintf(fp_stats, "Seed1 %" PRIu64 "\n", seed1);
		fprintf(fp_stats, "Seed2 %" PRIu64 "\n", seed2);
	}

	if (size == 1)
	{
		cudaSetDevice(gpu_id);
	}
	else
	{
		// Distribuite device to process (nodes)
		assignDeviceToProcess();
	}

	MPI_Barrier(MPI_COMM_WORLD);

	MaxLabel = -1;

	if (genGraph)
	{

		// Init the RMAT generator
		double initiator[4] = {.57, .19, .19, .05};

		// Init the Random generator
		if (genType == 1)
			initiator[0] = initiator[1] = initiator[2] = initiator[3] = .25;

		// Make the raw graph edges. (Reference code)
		// 32 BIT + nedges/2 = 8*nverts
		// Bitmask + nedges/2 + NUmax/2 = (8 + 8)*nverts
		if (rank == 0)
			checkMaxScale(SCALE, stdout, NUM_NVERTS_REQ);
		start = MPI_Wtime();
		make_graph(SCALE, (int64_t)(edgefactor * pow(2., SCALE)),
				   seed1, seed2, initiator, &nedges, &edges);
		stop = MPI_Wtime();
		make_graph_time = stop - start;
	}
	else
	{ // Load graph from file
		int64_t gnverts = 0;
		load_graph(filename_graph, &nedges, &edges, &gnverts);
		SCALE = ceil(log(gnverts) / log(2));
		fprintf(stdout, "[rank %d] SCALE=%d\n", rank, SCALE);
		//MPI_Bcast(&SCALE, 1, MPI_INT, 0, MPI_COMM_WORLD);
	}

	// Compute MaxLabel and MaxGlobalLabel for debug purposes
	global_scale = SCALE;
	global_edgefactor = edgefactor;

	for (cc = 0; cc < (2 * nedges); ++cc)
	{
		MaxLabel = (MaxLabel > edges[cc]) ? MaxLabel : edges[cc];
	}

	if (genGraph)
	{
		MaxGlobalLabel = (1 << global_scale) - 1;
	}
	else
	{
		MPI_Allreduce(&MaxLabel, &MaxGlobalLabel, 1, INT64_T_MPI_TYPE, MPI_MAX, MPI_COMM_WORLD);
		printf("MAX GLOBAL LEVEL %d\n", MaxGlobalLabel);
		MaxGlobalLabel += 1;
	}

	print_edges(edges, nedges, fp_struct, "MAIN");

	checkGlobalEdgeList(edges, nedges, stderr, "MAIN");

#ifndef DBG_LEVEL
	if (rank == 0)
	{
		fprintf(stdout, "scale=%d\n", global_scale);
		fprintf(stdout, "edgefactor=%f\n", edgefactor);
		fprintf(stdout, "nedges=%" PRI64 "\n", nedges);
		fprintf(stdout, "MaxLabel=%" PRI64 "\n", MaxLabel);
		fprintf(stdout, "size=%d\n", size);
	}
#endif
	PRINT_SPACE("MAIN:", fp_struct, "scale", global_scale);
	PRINT_SPACE("MAIN:", fp_struct, "edgefactor", (int)edgefactor);
	PRINT_SPACE("MAIN:", fp_struct, "nedges", nedges);
	PRINT_SPACE("MAIN:", fp_struct, "MaxLabel", MaxLabel);
	PRINT_TIME("MAIN:make_graph:", fp_struct, make_graph_time);

	MPI_Barrier(MPI_COMM_WORLD);

	// make g struct on device Kernel1
	adjlist hostgraph, dg; // our adjlist structure in host and device

	init_adjlist(&hostgraph);
	init_adjlist(&dg);

	start = MPI_Wtime();
#ifdef BUILD_STRUCT_DEVICE // Build csr structures on Device
	INT_T *appo_edges = (INT_T *)edges;
	INT_T appo_nedges = (INT_T)nedges;

	// Build the adjlist structure in device
	run_make_struct(appo_edges, appo_nedges, &dg);
#else // Build csr structures on Host
	csr_graph cg; // csr_graph structure according to reference graph500 code
	// Create the csr_graph structure using reference code IN HOST
	convert_graph_to_csr(nedges, edges, &cg);
	// Convert the csr_graph structure into the adjlist structure
	convert_csr_to_adj(&cg, &hostgraph, fp_struct);

#if defined(DBG_LEVEL) && (DBG_LEVEL > 0)
	check_adjlist(&hostgraph, fp_struct, "MAIN");
	//check_adjlist_against_reference(edges, nedges, &hostgraph, &cg, fp_struct);
#endif

#endif // BUILD_STRUCT_DEVICE

	stop = MPI_Wtime();
	make_struct_time = stop - start;
	if (rank == 0)
	{
		fprintf(stdout, "\n"
						"################################\n"
						"Make Struct:\n"
						"TIME=%f\n"
						"################################\n",
				make_struct_time);
	}
	// Every little thing is gonna be all right
#ifdef BUILD_STRUCT_DEVICE
	// Copy g structure from device to host
	copyAdjDeviceToHost(&hostgraph, &dg);
	free(appo_edges);
	appo_edges = NULL;
#else
	// Copy g structure from host to device
	copyAdjHostToDevice(&dg, &hostgraph);
	// Device memory allocated in the previous function
	current_allocated_size += hostgraph.nedges * sizeof(INT_T) + 3 * hostgraph.nverts * sizeof(INT_T);
	// Free csr_graph structure we do not need anymore
	free_csr_graph(&cg);
#endif

#if defined(DBG_LEVEL) && (DBG_LEVEL > 0)
	print_array_64t(hostgraph.edges, (int64_t)hostgraph.nedges, fp_struct, "MAIN");
#endif

	//Bitmask Structures on Host and Device
	mask h_bitmask, d_bitmask;
	init_bitmask(&h_bitmask);
	// Build bitmask unique edges on HOST
	make_bitmask(&hostgraph, &h_bitmask);

	// Build bitmask on device
	init_bitmask(&d_bitmask);
	build_bitmask_on_device(&h_bitmask, &d_bitmask, &dg);

#if defined(DBG_LEVEL) && (DBG_LEVEL > 0)
	print_device_array(dg.degree, dg.nverts, fp_struct, "dg.degree");
	print_device_array(d_bitmask.unique_edges, d_bitmask.m_nelems, fp_struct, "d_bitmask.unique_edges");
	print_device_array32(d_bitmask.pverts, dg.nverts, fp_struct, "d_bitmask.pverts");
	print_device_array32(d_bitmask.pedges, d_bitmask.p_nelems, fp_struct, "d_bitmask.pedges");
	print_device_array(d_bitmask.proc_offset, size + 1, fp_struct, "d_bitmask.proc_offset");
#endif

	current_allocated_size += h_bitmask.p_nelems * sizeof(INT32_T) + dg.nverts * sizeof(INT32_T) + h_bitmask.m_nelems * sizeof(INT32_T) + h_bitmask.m_nelems * sizeof(INT_T);

	// Free bitmask pointer array on HOST
	freemem(h_bitmask.unique_edges);

	prepStop = MPI_Wtime();
	make_struct_time = prepStop - prepStart;
	MPI_Reduce(&make_struct_time, &prepTime, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);

	if (rank == 0)
	{
		fprintf(stdout, "\n"
						"################################\n"
						"Full Preparation Time:\n"
						"TIME=%f\n"
						"################################\n",
				prepTime);
	}
	// Do BFS and validate results
	MPI_Barrier(MPI_COMM_WORLD);
	start = MPI_Wtime();
	run_make_stcon(&dg, &hostgraph, &d_bitmask);
	//print_device_array32(d_bitmask.pedges,d_bitmask.p_nelems, fp_bfs, "run_make_bfs_multi: ST-pedges");
	stop = MPI_Wtime();
	run_make_bfs_time = stop - start;
	PRINT_TIME("MAIN:make_bfs", fp_bfs, run_make_bfs_time);

	// Free in the sense of freedoom
	free_adjlist(&hostgraph);

	if (dbg_lvl > 0)
	{
		fclose(fp_struct);
		fp_struct = NULL;
		fclose(fp_bfs);
		fp_bfs = NULL;
	}

	cudaFree(dg.edges);
	cudaFree(dg.offset);
	cudaFree(dg.degree);
	if (rank == 0)
	{
		if (fp_stats != NULL)
			fclose(fp_stats);
	}
	// Every little thing is gonna be all right
	fprintf(stderr, "rank %d: eof\n", rank);

	MPI_Finalize();

	return 0;
}
